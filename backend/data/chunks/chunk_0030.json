{
  "chunk_id": "5e6cff41-31a6-477d-9e0f-8b0478bbae48",
  "page_url": "https://local-content/module3/chapter3x",
  "heading_path": "",
  "content_raw": "import Prerequisites from '@site/src/components/Prerequisites'; import LearningObjective from '@site/src/components/LearningObjective'; import Outcome from '@site/src/components/Outcome'; import SuccessCriteria from '@site/src/components/SuccessCriteria'; import Exercise from '@site/src/components/Exercise'; import Checkpoint from '@site/src/components/Checkpoint'; Chapter 3: Vision-Language-Action (VLA) Models in Robotics This chapter explores the use of Vision-Language-Action (VLA) models in robotics. <Prerequisites> <p>You should have a basic understanding of deep learning, computer vision, and natural language processing.</p> </Prerequisites> <LearningObjective> <ul> <li>Understand what a Vision-Language-Action (VLA) model is.</li> <li>Learn how VLAs can be used to control robots with natural language.</li> <li>Be introduced to the challenges of grounding language in the real world.</li> </ul> </LearningObjective> What are VLAs? Vision-Language-Action (VLA) models are a new frontier in artificial intelligence. They are large-scale models, similar to the Large Language Models (LLMs) that power chatbots, but with a crucial difference: they are multimodal and embodied. - Vision: VLAs can take images or video streams as input, allowing them to \"see\" the world. - Language: They can understand and generate natural language, enabling them to communicate with humans. - Action: They can output actions to control a robot, such as motor commands. Natural Language Control of Robots VLAs open up the exciting possibility of controlling robots with simple, natural language commands. Instead of writing complex code, you could just tell a robot, \"Please pick up the red ball and put it in the blue box.\" The VLA would then be responsible for parsing this command, identifying the red ball and the blue box in its camera feed, and generating the sequence of motor commands to execute the task. The Grounding Problem One of the biggest challenges in developing VLAs is the grounding problem. This is the problem of connecting the symbols of language (like the word \"red\") to the real world (the actual color red as perceived by a camera). For a VLA to be effective, it must learn to ground language in its sensory experience of the world. <Checkpoint> <p>What makes a VLA different from a standard Large Language Model (LLM)?</p> </",
  "token_count": 500,
  "overlap_with_previous": "",
  "overlap_with_next": "",
  "position_in_page": 0,
  "metadata": {},
  "status": "PENDING"
}